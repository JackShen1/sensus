{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Sentiment Analysis with LSTMs</h1>\n",
    "\n",
    "First, we need to create word vectors. For simplicity, we will use a pretrained Word2Vec model\n",
    "with Ukrainian words-vectors, each of which has a dimension of 300.\n",
    "\n",
    "This sample of word vectors was created on the basis of fiction literature. We chose the lematized version of this model because we already have our sample, which we processed in the previous part, which would fit perfectly here.\n",
    "\n",
    "The model can be found on [this website](https://lang.org.ua/uk/models/).\n",
    "\n",
    "To get started, let's download the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the sample data we processed in the previous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.pql', 'rb') as f:\n",
    "     docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['працювати', 'автор', 'секція', 'наркоманія', 'чиказький', 'поліцейський', 'управління', 'бути', 'ласка', 'дозволяти', 'сприйняття', 'правда', 'автор', 'думка', 'чиказький', 'відділ', 'поліція', 'чоловік', 'жінка', 'гордо', 'служити', 'автор', 'стверджувати', 'секція', 'боротьба', 'наркотик', 'політик', 'сприяти', 'занепад', 'думати', 'процвітати', 'практик', 'використовувати', 'користь', 'бачити', 'егоїстичний', 'незрілий', 'юнак', 'думати', 'перший', 'якщо', 'автор', 'вважати', 'командир', 'наркологічний', 'секція', 'огидний', 'вживання', 'наркотик', 'невдалий', 'аналіз', 'сеча', 'помилятися', 'якщо', 'читати', 'книга', 'ласка', 'звинувачувати', 'неприємний', 'історія', 'людина'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load our word2vec model. \n",
    "\n",
    "This may take some time, as the model contains approximately 58 500 words, so we will get a 58500 x 300 embedding matrix that contains all the values of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('fiction.lowercased.lemmatized.word2vec.300d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a list of all the words from our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything is loaded correctly, we can look at the dimensions of the dictionary list and the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['і', 'не', 'на', 'в', 'що', 'з', 'а', 'я', 'й', 'він', 'до', 'бути', 'як', 'у', 'та', 'вона', 'його', 'за', 'ти', 'вони', 'це', 'свій', 'той', 'так', 'ж', 'ми', 'ви', 'то', 'але', 'ще', 'такий', 'по', 'її', 'себе', 'могти', 'вже', 'який', 'один', 'сам', 'все', 'про', 'сказати', 'мати', 'чи', 'від', 'знати', 'рука', 'око', 'мій', 'коли', 'те', 'тільки', 'щоб', 'ні', 'цей', 'всі', 'тут', 'бо', 'там', 'б', 'хто', 'де', 'для', 'голова', 'казати', 'тепер', 'люди', 'під', 'хотіти', 'i', 'и', 'із', 'бачити', 'стояти', 'о', 'піти', 'земля', 'щось', 'час', 'якийсь', 'наш', 'слово', 'тоді', 'треба', 'булий', 'великий', 'аж', 'раз', 'перед', 'хоч', 'над', 'би', 'навіть', 'хата', 'пан', 'уже', 'ну', 'дивитися', 'йти', 'думати'] \n",
      "\n",
      "Total words: 58492 \n",
      "\n",
      "Word-Vectors shape: (58492, 300)\n"
     ]
    }
   ],
   "source": [
    "print(words[:100], \"\\n\\nTotal words:\", len(words), \"\\n\\nWord-Vectors shape:\", model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find a word like \"гарний\" in our word list and then access the corresponding vector through the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.076125  0.553508 -0.843871  0.210367 -1.051    -0.456979 -0.071229\n",
      "  0.295579 -0.239752 -1.436497  1.411778  1.076445 -1.785111 -0.305013\n",
      " -0.775204 -1.181594 -0.742579  0.024011 -1.269905 -0.540188  0.451396\n",
      "  0.782721 -0.309379 -1.417217  0.702385 -1.744658  0.032138 -0.098305\n",
      " -0.99159  -0.109905  0.069233  0.37022  -0.247679  0.632742 -0.693778\n",
      "  0.076653  0.531913 -1.158516  0.188859 -0.357537  0.296323  1.144795\n",
      "  0.458457 -0.693163 -0.268378  0.105673  0.170314  1.165459  0.258425\n",
      "  0.550712 -0.324439  0.132404 -0.201105 -0.525112  0.103556 -1.297686\n",
      " -0.550728 -0.309694 -0.371412  0.007037 -0.263882  0.096639 -0.861688\n",
      "  0.504681 -0.304385  0.842935  0.014222  0.039818 -0.442853 -0.848775\n",
      " -0.329982  0.096731 -0.132125 -0.258058  0.65424  -0.669513  0.765716\n",
      " -0.695802 -0.299159 -0.648143  2.504469  0.17683  -0.698639  0.415211\n",
      " -0.021573  0.740519 -0.163555  0.39969  -0.410044  0.133693 -0.452481\n",
      " -1.250075  1.471488  0.030436  0.728958  1.194957  0.483038  0.620524\n",
      " -0.077783 -0.105126  0.324508 -0.831754 -1.0763   -1.517556 -0.330903\n",
      "  1.210441  0.987843  0.738817  0.430181  0.53489  -0.535614 -0.625942\n",
      "  0.157871  1.078086 -0.378683  0.385601 -0.267464 -0.01076  -1.047701\n",
      " -0.123989  0.213617 -0.064    -0.58727   0.26745   0.78272   1.755387\n",
      "  1.231713 -0.349373  0.212132  0.740103  0.406082  0.388426  0.161591\n",
      "  0.156761  0.093474 -0.842644 -0.547395 -0.521667  0.142774  0.319378\n",
      "  0.49371   1.360553  0.262072 -0.605569 -0.645101  0.493505 -0.322506\n",
      "  0.733869 -0.388189  0.699744 -0.54217   0.951679  1.285943 -0.356164\n",
      " -0.358545  1.384654 -1.582798  1.246367 -0.029445 -0.575798 -0.299138\n",
      "  0.262088  0.442177  0.469577  0.169937 -0.04943  -0.767963  0.061723\n",
      " -0.176479  1.838898  0.502406 -1.571039  0.397811 -0.343501  0.924943\n",
      " -0.503262  0.484259 -0.771695  0.731097  0.067368  0.41311  -0.490052\n",
      "  0.358071  0.011889  0.417015  0.176896  0.538514 -1.17616   0.701982\n",
      " -0.695039 -1.049698  0.809101  0.368496 -0.557108  0.69461  -0.415916\n",
      "  0.814222  0.418163 -0.862628 -0.720502  0.030916  0.98892   0.015276\n",
      "  0.483669 -0.06602  -0.749127  0.745763 -0.375981 -1.392793 -0.647\n",
      "  0.355687  0.10293  -0.942756  0.311517 -0.481752  0.091985  0.281321\n",
      "  0.428452 -0.548031 -1.156749  0.914364 -0.006886  0.68007  -1.546817\n",
      "  0.292327  0.936825  0.167396  1.185384  0.570955 -0.276981  0.861155\n",
      "  0.183653 -1.405156 -0.680503  0.655626  1.077908  0.85671  -0.60104\n",
      " -0.839491 -0.202422 -0.555484 -0.456561 -0.361716  1.646824 -0.251199\n",
      "  0.639167  1.17727   0.823772 -0.544036  0.074212  0.734082 -0.663308\n",
      " -0.767836 -0.060576 -0.044934 -0.378725 -0.118548  0.039818  0.476696\n",
      "  0.057378  0.472461 -0.449803  0.544326 -0.224758 -0.863157  0.719\n",
      "  0.991895  0.690479  0.543221 -0.197186  0.272502  0.141123  1.120919\n",
      "  0.317952 -0.116973 -1.924045  0.589315 -0.304467  0.039616  0.468136\n",
      " -0.697053 -1.141324 -0.751758  0.812557 -0.034631  0.477412 -0.262882\n",
      " -0.359381 -0.827246 -0.025136 -0.951737 -0.445382  0.745683 -0.221026\n",
      " -0.742218 -0.186806  1.000658  0.648798 -0.574093  0.483465]\n"
     ]
    }
   ],
   "source": [
    "print(model['гарний'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
