{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LSTM Model</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's download the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's write a style for alignment in the middle of all graphs, images, etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Preparation of input data</h2> \n",
    "\n",
    "Firstly, we will load the sample data we processed in the first part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.pql', 'rb') as f:\n",
    "     docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load our word2vec model. \n",
    "\n",
    "This may take some time, as the model contains 325 250  words, so we will get a 325 250 x 300 embedding matrix that contains all the values of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('ubercorpus.lowercased.lemmatized.word2vec.300d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get a list of all the words from our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything is loaded correctly, we can look at the dimensions of the dictionary list and the embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total words:\", len(words), \"\\n\\nWord-Vectors shape:\", model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of our sample for training will be `(H, 300, N)`, where:\n",
    "* `H` - number of samples;\n",
    "* `300` - dimension of each word; \n",
    "* `N` - number of words in each sentence.\n",
    "\n",
    "Let's analyze how many words are usually found in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word, min_word = 0, 100\n",
    "word50, word100, word200, word300, word400  = 0, 0, 0, 0, 0\n",
    "reviews_len = []\n",
    "\n",
    "for review, state in docs:\n",
    "    reviews_len.append(len(review))\n",
    "\n",
    "    if len(review) > max_word: max_word = len(review)\n",
    "    if len(review) < min_word: min_word = len(review)\n",
    "\n",
    "    if len(review) > 50: word50 += 1\n",
    "    if len(review) > 100: word100 += 1\n",
    "    if len(review) > 200: word200 += 1\n",
    "    if len(review) > 300: word300 += 1\n",
    "    if len(review) > 400: word400 += 1\n",
    "    \n",
    "print(\"Average number of words in the review:\", int(sum(reviews_len)/len(reviews_len)))\n",
    "print(\"\\nMaximum review length:\", max_word, \"\\nMinimum review length:\", min_word)\n",
    "print(\"\\nReview with more than 50 words:\", word50, \n",
    "      \"\\nReview with more than 100 words:\", word100,\n",
    "      \"\\nReview with more than 200 words:\", word200, \n",
    "      \"\\nReview with more than 300 words:\", word300,\n",
    "      \"\\nReview with more than 400 words:\", word400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the frequency of words in the review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(reviews_len, bins=50)\n",
    "plt.axis([0, 600, 0, 1500])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Sequence Length');\n",
    "plt.title(\"Average number of words\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next function `fix_review_len()` will do a useful job, this function is designed to fix the size of reviews to a fixed size to feed them into a neural network with reviews of a certain length. Reviews whose length is less than fixed will be extended by zeros. This process does not affect the algorithm and reviews longer than the specified length will be truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_review_len(review, length):\n",
    "    if len(review) > length:\n",
    "        review = review[:length]\n",
    "    elif len(review) < length:\n",
    "        for i in range(length - len(review)):\n",
    "            zeros = [0] * 300\n",
    "            review.append(zeros)\n",
    "    return review\n",
    "\n",
    "example = [3, 1, 2, 4, 5]\n",
    "example = fix_review_len(example, 7)\n",
    "print(example) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's slightly improve our `sent_embed()` function from previous part and then update our dataset with word vectors.\n",
    "\n",
    "Based on the histogram data, as well as the average number of words in the files, we can say with confidence that most reviews will have less than 100 words, which is the maximum value of the length of the sequence that we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_embed(words, docs):\n",
    "    x_sent_embed, y_sent_embed = [], [] \n",
    "    \n",
    "    max_seq_len = 100\n",
    "    \n",
    "    # recover the embedding of each sentence with the average of the vector that composes it\n",
    "    # sent - sentence, state - state of the sentence (pos/neg)\n",
    "    for sent, state in docs:\n",
    "        # average embedding of all words in a sentence\n",
    "        sent_embed = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # if word is present in the dictionary - add its vector representation\n",
    "                sent_embed.append(model[word])\n",
    "            except KeyError:\n",
    "                # if word is not in the dictionary - add a zero vector\n",
    "                sent_embed.append([0] * 300)\n",
    "        \n",
    "        # add a sentence vector to the list\n",
    "        sent_embed = fix_review_len(sent_embed, max_seq_len)\n",
    "        x_sent_embed.append(sent_embed)\n",
    "        \n",
    "        # add a label to y_sent_embed\n",
    "        if state == 'pos': y_sent_embed.append(1)\n",
    "        elif state == 'neg': y_sent_embed.append(0)\n",
    "            \n",
    "    return x_sent_embed, y_sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = sent_embed(words, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "print(\"Shape of X:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Split Corpus</h3>\n",
    "\n",
    "Now, for further work, we will divide our corpus for training, testing and development sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train dev\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of x_train:', len(x_train), '| Length of y_train:', len(y_train))\n",
    "print('Length of x_test:  ', len(x_test), '| Length of y_test: ', len(y_test))\n",
    "print('Length of x_val:   ', len(x_val), '| Length of y_val:  ', len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x_train set:\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">LSTM Model (Batch Size)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras's benefit is that it is built on top of symbolic mathematical libraries such as TensorFlow and Theano for fast and efficient computation. This is needed with large neural networks.\n",
    "\n",
    "A downside of using these efficient libraries is that you must define the scope of your data upfront and for all time. Specifically, the batch size.\n",
    "\n",
    "The batch size limits the number of samples to be shown to the network before a weight update can be performed. This same limitation is then imposed when making predictions with the fit model.\n",
    "\n",
    "Specifically, the batch size used when fitting your model controls how many predictions you must make at a time.\n",
    "\n",
    "This is often not a problem when you want to make the same number predictions at a time as the batch size used during training.\n",
    "\n",
    "This does become a problem when you wish to make fewer predictions than the batch size. For example, you may get the best results with a large batch size but are required to make predictions for one observation at a time on something like a time series or sequence problem.\n",
    "\n",
    "This is why it may be desirable to have a different batch size when fitting the network to training data than when making predictions on test data or new input data.\n",
    "\n",
    "Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Setting Up and Creating the Network</h3>\n",
    "\n",
    "The network has one input, a hidden layer with 100 units, and an output layer with 1 unit.\n",
    "\n",
    "A mean squared error optimization function is used for this problem with the efficient ADAM optimization algorithm.\n",
    "\n",
    "So now let's configure and create the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# configure network\n",
    "n_neurons = 100\n",
    "n_epoch = 20\n",
    "n_batch = 1\n",
    "\n",
    "# design network\n",
    "model_batch = Sequential()\n",
    "model_batch.add(LSTM(n_neurons, input_shape=(x.shape[1], x.shape[2])))\n",
    "model_batch.add(Dense(1, activation='sigmoid'))\n",
    "model_batch.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print(model_batch.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch_hist = model_batch.fit(x_train, np.asarray(y_train), batch_size=n_batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_batch = model_batch.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel loss:\", str(round(score_batch[0] * 100, 2)) + '%', \n",
    "      \"\\nModel Accuracy:\",str(round(score_batch[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.plot(model_batch_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not entirely satisfactory, but let's save this model and try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_batch.save('LSTM-Batch-Model.h5')  # save model\n",
    "del model_batch  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_batch = load_model('LSTM-Batch-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">LSTM Model (Mini-Batch Gradient Descent)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.\n",
    "\n",
    "Mini batch algorithm is the most favorable and widely used algorithm that makes precise and faster results using a batch of `m` training examples. In mini batch algorithm rather than using  the complete data set, in every iteration we use a set of `m` training examples called batch to compute the gradient of the cost function. Common mini-batch sizes range between 32 and 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Setting Up and Creating the Network</h3>\n",
    "\n",
    "We will use an LSTM network fit for 50 epochs and with batch size = 32.\n",
    "\n",
    "The network has one input, a hidden layer with 100 units, and an output layer with 1 unit.\n",
    "\n",
    "A mean squared error optimization function is used for this problem with the efficient ADAM optimization algorithm.\n",
    "\n",
    "`recurrent_dropout`: float between 0 and 1. Fraction of the input units to drop for recurrent connections. \n",
    "\n",
    "`dropout`: float between 0 and 1. Fraction of the input units to drop for input gates.\n",
    "\n",
    "\n",
    "So now let's configure and create the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dropout\n",
    "\n",
    "# configure network\n",
    "n_neurons = 100\n",
    "n_epoch = 50\n",
    "n_batch = 32\n",
    "\n",
    "# design network\n",
    "model_mini = Sequential()\n",
    "model_mini.add(LSTM(n_neurons, input_shape=(x.shape[1], x.shape[2]), dropout=0.2, recurrent_dropout=0.2))\n",
    "model_mini.add(Dense(1, activation='sigmoid'))\n",
    "model_mini.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print(model_mini.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini_hist = model_mini.fit(x_train, np.asarray(y_train), batch_size=n_batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini_score = model_mini.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel loss:\", str(round(model_mini_score[0] * 100, 2)) + '%', \n",
    "      \"\\nModel Accuracy:\",str(round(model_mini_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_mini_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good value for the model, we can use it. Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini.save('LSTM-Mini-Batch-Model.h5')  # save model\n",
    "del model_mini  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_mini = load_model('LSTM-Mini-Batch-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mini.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Stacked LSTM Layers</h2>\n",
    "\n",
    "Now we will try the model with a 3 stacked LSTM layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network\n",
    "n_neurons = 100\n",
    "n_epoch = 20\n",
    "n_batch = 32\n",
    "\n",
    "# design network\n",
    "model3L = Sequential()\n",
    "\n",
    "model3L.add(LSTM(n_neurons, return_sequences=True, input_shape=(x.shape[1], x.shape[2])))\n",
    "model3L.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model3L.add(LSTM(32))  # return a single vector of dimension 32\n",
    "\n",
    "model3L.add(Dense(1, activation='sigmoid'))\n",
    "model3L.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "print(model3L.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L_hist = model3L.fit(x_train, np.asarray(y_train), batch_size=n_batch, epochs=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L_score = model3L.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel loss:\", str(round(model3L_score[0] * 100, 2)) + '%', \n",
    "      \"\\nModel Accuracy:\",str(round(model3L_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_mini_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L.save('LSTM-3L-Model.h5')  # save model\n",
    "del model3L  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model3L = load_model('LSTM-3L-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3L.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">CNN + LSTM</h2>\n",
    "\n",
    "This approach is based entirely on [this work](https://www.aclweb.org/anthology/P16-2037.pdf). As indicated in the work:\n",
    "\n",
    "<cite>\n",
    "    \n",
    "    Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valencearousal (VA) space. \n",
    "    \n",
    "    Compared to the categorical approach that focuses on sentiment classification such as binary classification (i.e., positive and negative), the dimensional approach can provide more fine-grained sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts: regional CNN and LSTM to predict the VA ratings of texts. \n",
    "   \n",
    "    Unlike a conventional CNN which considers a whole text as input, the proposed regional CNN uses an individual sentence as a region, dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction. Such regional information is sequentially integrated across regions using LSTM for VA prediction. \n",
    "\n",
    "</cite>\n",
    "\n",
    "In short, word vectors of vocabulary words are trained from a large corpus using the word2vec toolkit. For each given text, the  regional CNN model uses the sentence as a region to divide the text into R-domains, ie r1, ..., ri rj, rk, ..., rR. In each region, useful affective functions can be removed when word vectors pass sequentially through the convolutional layer and the  maxpooling  layer. Then such local (regional) features are sequentially integrated between regions using LSTM to construct a text vector for VA prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# configure network\n",
    "n_epoch = 20\n",
    "n_batch = 32\n",
    "\n",
    "# design network\n",
    "model_CNN_LSTM = Sequential()\n",
    "\n",
    "model_CNN_LSTM.add(Conv1D(filters=100, kernel_size=3, input_shape=(x.shape[1], x.shape[2]), padding='same', activation='relu'))\n",
    "model_CNN_LSTM.add(MaxPooling1D(pool_size=2))\n",
    "model_CNN_LSTM.add(LSTM(100))\n",
    "model_CNN_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_CNN_LSTM.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model_CNN_LSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_LSTM_hist = model_CNN_LSTM.fit(x_train, np.asarray(y_train), epochs=n_epoch, batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_LSTM_score = model_CNN_LSTM.evaluate(x_val, np.asarray(y_val), batch_size=n_batch)\n",
    "print(\"\\nModel loss:\", str(round(model_CNN_LSTM_score[0] * 100, 2)) + '%', \n",
    "      \"\\nModel Accuracy:\",str(round(model_CNN_LSTM_score[1] * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model_mini_hist.history['loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_LSTM.save('LSTM-CNN-Model.h5')  # save model\n",
    "del model_CNN_LSTM  # delete existing model\n",
    "\n",
    "# returns a compiled model, identical to the previous one\n",
    "model_CNN_LSTM = load_model('LSTM-CNN-Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Testing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to check the test data with all but one of the test examples to see if the model can predict it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of test data (except 1):\", len(x_test[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test = model_CNN_LSTM.evaluate(x_test[:-1], np.asarray(y_test[:-1]))\n",
    "print(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test_one = model_CNN_LSTM.evaluate(x_test[-1:], np.asarray(y_test[-1:]))\n",
    "print(score_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = model_CNN_LSTM.predict_classes(x_test[:-1])\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_one = model_CNN_LSTM.predict_classes(x_test[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted value:\", predict_test_one[0][0], \"\\nActual value:\", y_test[-1:][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction works great, now let's visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Visualization of Classification Report</h2>\n",
    "\n",
    "We will need our method from previous part to visualize our data, so we will use them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(classification_report, title='Classification Report', cmap='RdBu'):\n",
    "    \n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes, plotMat, support, class_names = [], [], [], []\n",
    "    \n",
    "    for line in lines[2 : (len(lines) - 5)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        plotMat.append(v)\n",
    "\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    \n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    \n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    \n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    fig, ax = plt.subplots()\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle='dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    # add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # add text in each cell \n",
    "    show_val(c)\n",
    "\n",
    "    # proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(cm_to_inch(figure_width, figure_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_val(pc, fmt=\"%.2f\", **kw):\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.axes\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_to_inch(*dim):\n",
    "    inch = 2.54\n",
    "    return tuple(i/inch for i in dim[0]) if type(dim[0]) == tuple else tuple(i/inch for i in dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">Comparison of Models</h2> \n",
    "\n",
    "A useful tool when predicting the probability of a binary outcome is the Receiver Operating Characteristic curve, or ROC curve.\n",
    "\n",
    "It is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0. Put another way, it plots the false alarm rate versus the hit rate.\n",
    "\n",
    "The true positive rate is calculated as the number of true positives divided by the sum of the number of true positives and the number of false negatives. It describes how good the model is at predicting the positive class when the actual outcome is positive.\n",
    "\n",
    "The false positive rate is calculated as the number of false positives divided by the sum of the number of false positives and the number of true negatives.\n",
    "\n",
    "It is also called the false alarm rate as it summarizes how often a positive class is predicted when the actual outcome is negative.\n",
    "\n",
    "To make this clear:\n",
    "* Smaller values on the x-axis of the plot indicate lower false positives and higher true negatives.\n",
    "* Larger values on the y-axis of the plot indicate higher true positives and lower false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "batch_prediction = (model_batch.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print('Classification Report LSTM Batch Model:\\n', classification_report(y_test, batch_prediction))\n",
    "\n",
    "plot_classification_report(classification_report(y_test, batch_prediction), title='LSTM Batch Classification Report', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_prediction = (model_mini.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print('Classification Report LSTM Mini-Batch Model:\\n', classification_report(y_test, mini_prediction))\n",
    "\n",
    "plot_classification_report(classification_report(y_test, mini_prediction), title='LSTM Mini-Batch Classification Report', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_layer_prediction = (model3L.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print('Classification Report LSTM Stacked Layers Model:\\n', classification_report(y_test, model3L))\n",
    "\n",
    "plot_classification_report(classification_report(y_test, stack_layer_prediction), title='LSTM Stacked Layers Classification Report', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cnn_prediction = (model_CNN_LSTM.predict(x_test) > 0.5).astype(\"int32\")\n",
    "print('Classification Report CNN + LSTM Model:\\n', classification_report(y_test, model_CNN_LSTM))\n",
    "\n",
    "plot_classification_report(classification_report(y_test, lstm_cnn_prediction), title='CNN + LSTM Classification Report', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fprB, tprB, thresholdsB = metrics.roc_curve(y_test, model_batch.predict(x_test))\n",
    "fprMB, tprMB, thresholdsMB = metrics.roc_curve(y_test, model_mini.predict(x_test))\n",
    "fpr3L, tpr3L, thresholds3L = metrics.roc_curve(y_test, model3L.predict(x_test))\n",
    "fprCNN, tprCNN, thresholdsCNN = metrics.roc_curve(y_test, model_CNN_LSTM.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linewidth = 2\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(fprB, tprB, color='darkorange', lw=linewidth, label='ROC Curve LSTM Batch (AUC = %0.3f)' % auc(fprB, tprB))\n",
    "plt.plot(fprMB, tprMB, color='black', lw=linewidth, label='ROC Curve LSTM Mini Batch(AUC = %0.3f)' % auc(fprMB, tprMB))\n",
    "plt.plot(fpr3L, tpr3L, color='forestgreen', lw=linewidth, label='ROC Curve LSTM Stacked Layers (AUC = %0.3f)' % auc(fpr3L, tpr3L))\n",
    "plt.plot(fprCNN, tprCNN, color='mediumpurple', lw=linewidth, label='ROC Curve  CNN + LSTM (AUC = %0.3f)' % auc(fprCNN, tprCNN))\n",
    "plt.plot([0, 1], [0, 1], color='royalblue', lw=linewidth, linestyle='--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Plots')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
